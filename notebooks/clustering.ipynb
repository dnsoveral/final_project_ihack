{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12788ba7-5ae1-4dd9-9a1e-2ce5416e043d",
   "metadata": {
    "tags": []
   },
   "source": [
    "![logo_ironhack_blue 7](https://user-images.githubusercontent.com/23629340/40541063-a07a0a8a-601a-11e8-91b5-2f13e4e6b441.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65deff48-e2c1-4338-a5c1-907d9f163405",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e8f13d-76ff-49c8-8687-a535dc513697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "import scipy.cluster.hierarchy as sch\n",
    "import spacy\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "pd.get_option(\"display.max_columns\")\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af65872-d65a-4db9-83d3-bd106070fdbd",
   "metadata": {},
   "source": [
    "# Loading CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4578ac21-ad7a-4a15-9962-b378084cd8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes = pd.read_csv('../data/raw/recipes_one_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5750d0-726c-4073-afdb-2759b3a15405",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e4f743-5c92-4502-a512-8d91a0cc3ba1",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d88170-7e3b-4c40-9812-9fe5cf1babca",
   "metadata": {},
   "source": [
    "## Clustering Time + Difficulty + Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491a686c-349c-400d-a8e2-79d0febd5524",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "recipes['cost_encoded'] = label_encoder.fit_transform(recipes['cost'])\n",
    "recipes['time_encoded'] = label_encoder.fit_transform(recipes['time(min)'])\n",
    "recipes['difficulty_encoded'] = label_encoder.fit_transform(recipes['difficulty'])\n",
    "recipes['meal_class_encoded'] = label_encoder.fit_transform(recipes['meal_class'])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "recipes[['cost_encoded', 'time_encoded', 'difficulty_encoded', 'meal_class_encoded']] = scaler.fit_transform(recipes[['cost_encoded', 'time_encoded', 'difficulty_encoded', 'meal_class_encoded']])\n",
    "\n",
    "# Save the StandardScaler for scaling the features\n",
    "with open('../scalers/cluster_scaler.pkl', 'wb') as scaler_file:\n",
    "    pickle.dump(scaler, scaler_file)\n",
    "\n",
    "# Save the LabelEncoder for categorical features\n",
    "with open('../encoders/encoder.pkl', 'wb') as label_encoder_file:\n",
    "    pickle.dump(label_encoder, label_encoder_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f21a6e-7cfd-471a-9729-f9a0c82ec1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features you want to use for clustering\n",
    "features = ['time_encoded', 'difficulty_encoded', 'cost_encoded', 'meal_class_encoded']\n",
    "\n",
    "# Initialize a dictionary to store silhouette scores for each feature\n",
    "silhouette_scores = {}\n",
    "\n",
    "cluster_range = range(2, 20)\n",
    "\n",
    "# Loop through each feature and each number of clusters\n",
    "for feature in features:\n",
    "    silhouette_scores[feature] = []\n",
    "    \n",
    "    for num_clusters in cluster_range:\n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=24, n_init=50)  \n",
    "        recipes['cluster'] = kmeans.fit_predict(recipes[[feature]])\n",
    "        silhouette = silhouette_score(recipes[[feature]], recipes['cluster'])\n",
    "        silhouette_scores[feature].append(silhouette)\n",
    "\n",
    "# Plot the silhouette scores for each feature\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for feature in features:\n",
    "    plt.plot(cluster_range, silhouette_scores[feature], marker='o', linestyle='-', label=feature)\n",
    "\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.title('Silhouette Score for Optimal Cluster Number')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Choose the number of clusters with the highest silhouette score for each feature\n",
    "optimal_num_clusters = {}\n",
    "\n",
    "for feature in features:\n",
    "    optimal_num_clusters[feature] = np.argmax(silhouette_scores[feature]) + 2\n",
    "\n",
    "# Print the optimal number of clusters for each feature\n",
    "print(\"Optimal Number of Clusters (Time Labels):\", optimal_num_clusters['time_encoded'])\n",
    "print(\"Optimal Number of Clusters (Difficulty):\", optimal_num_clusters['difficulty_encoded'])\n",
    "print(\"Optimal Number of Clusters (Cost):\", optimal_num_clusters['cost_encoded'])\n",
    "print(\"Optimal Number of Clusters (Meal Class):\", optimal_num_clusters['meal_class_encoded'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44d64f6-c8fc-4a75-a8cb-b1db950799b4",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde780be-603f-4cda-bd4b-896b19fa7741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the 'meal_class' feature using one-hot encoding\n",
    "encoder = OneHotEncoder(sparse=False)  # Use sparse=False to get a dense array\n",
    "meal_class_encoded = encoder.fit_transform(recipes[['meal_class']])\n",
    "\n",
    "# Perform hierarchical clustering with the encoded 'meal_class'\n",
    "num_clusters = optimal_num_clusters['meal_class_encoded']\n",
    "clustering = AgglomerativeClustering(n_clusters=num_clusters, linkage='ward')\n",
    "meal_class_clusters = clustering.fit_predict(meal_class_encoded)\n",
    "\n",
    "# Combine the cluster labels for 'meal_class' with other features\n",
    "recipes['combined_clusters'] = recipes['time_encoded'].astype(str) + \"_\" + recipes['difficulty_encoded'].astype(str) + \"_\" + recipes['cost_encoded'].astype(str) + \"_\" + meal_class_clusters.astype(str)\n",
    "\n",
    "# Remove the trailing underscore\n",
    "recipes['combined_clusters'] = recipes['combined_clusters'].str.rstrip('_')\n",
    "\n",
    "# Create a dictionary to map combined cluster labels to unique integers\n",
    "cluster_label_to_int = {label: idx for idx, label in enumerate(recipes['combined_clusters'].unique())}\n",
    "\n",
    "# Map the combined cluster labels to integers and create a new column\n",
    "recipes['combined_clusters_int'] = recipes['combined_clusters'].map(cluster_label_to_int)\n",
    "\n",
    "# Save the encoder to a file using pickle\n",
    "encoder_filename = '../encoders/onehot_encoder_agglomerative.pkl'\n",
    "with open(encoder_filename, 'wb') as encoder_file:\n",
    "    pickle.dump(encoder, encoder_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a376c80d-05ed-4613-bf42-4da8c7d7542b",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e34945-672f-4e40-b467-f7ef53aa8471",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_unique_clusters = recipes['combined_clusters_int'].nunique()\n",
    "\n",
    "print(\"Number of unique combined integer clusters:\", num_unique_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df7a38-dd5c-4a93-be6c-f722e8ff53de",
   "metadata": {},
   "source": [
    "# Encoding for Cosine Similarity Inside Each Meal Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5638aae7-d441-473d-9cfb-b4632d9be206",
   "metadata": {},
   "source": [
    "## Vectorizing and Finding Cosine Similarity in Ingredients Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d84a12-5cac-4966-b345-8c0ea3eb0a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform the ingredients_combined column into TF-IDF vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(recipes['ingredients_combined'])\n",
    "\n",
    "# Create a new column 'ingredients_cosine' with NaN values\n",
    "recipes['ingredients_cosine'] = pd.Series(dtype='float64')\n",
    "\n",
    "# Calculate cosine similarity between recipes and the average vector in the same meal class\n",
    "for meal_class in recipes['meal_class'].unique():\n",
    "    indices = recipes[recipes['meal_class'] == meal_class].index\n",
    "    avg_tfidf_vector = tfidf_matrix[indices].mean(axis=0)\n",
    "    \n",
    "    # Convert tfidf_matrix to a numpy array\n",
    "    tfidf_matrix_array = tfidf_matrix.toarray()\n",
    "    \n",
    "    # Convert avg_tfidf_vector to a numpy array\n",
    "    avg_tfidf_vector_array = np.asarray(avg_tfidf_vector).reshape(1, -1)\n",
    "    \n",
    "    # Calculate cosine similarities\n",
    "    cosine_similarities = cosine_similarity(tfidf_matrix_array[indices], avg_tfidf_vector_array)\n",
    "    \n",
    "    # Fill the 'ingredients_cosine' column with the cosine similarity values\n",
    "    for i, index_i in enumerate(indices):\n",
    "        recipes.at[index_i, 'ingredients_cosine'] = cosine_similarities[i][0]\n",
    "\n",
    "recipes.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac1bb32-e32d-4d95-9c32-393463c85a3a",
   "metadata": {},
   "source": [
    "## Vectorizing and Finding Cosine Similarity in Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57202389-39b7-4b4d-bdef-7a62f2d1bf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the preparations column into TF-IDF vectors\n",
    "tfidf_matrix = vectorizer.fit_transform(recipes['preparations'])\n",
    "\n",
    "# Create a new column 'preparations_cosine' with NaN values\n",
    "recipes['preparations_cosine'] = pd.Series(dtype='float64')\n",
    "\n",
    "# Calculate cosine similarity between recipes and the average vector in the same meal class\n",
    "for meal_class in recipes['meal_class'].unique():\n",
    "    indices = recipes[recipes['meal_class'] == meal_class].index\n",
    "    avg_tfidf_vector = tfidf_matrix[indices].mean(axis=0)\n",
    "    \n",
    "    # Convert tfidf_matrix to a numpy array\n",
    "    tfidf_matrix_array = tfidf_matrix.toarray()\n",
    "    \n",
    "    # Convert avg_tfidf_vector to a numpy array\n",
    "    avg_tfidf_vector_array = np.asarray(avg_tfidf_vector).reshape(1, -1)\n",
    "    \n",
    "    # Calculate cosine similarities\n",
    "    cosine_similarities = cosine_similarity(tfidf_matrix_array[indices], avg_tfidf_vector_array)\n",
    "    \n",
    "    # Fill the 'preparations_cosine' column with the cosine similarity values\n",
    "    for i, index_i in enumerate(indices):\n",
    "        recipes.at[index_i, 'preparations_cosine'] = cosine_similarities[i][0]\n",
    "\n",
    "recipes.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f021cfbf-37ea-4377-b736-88e60522bc32",
   "metadata": {},
   "source": [
    "## Scaling and Finding Cosine Similarity in Time(Min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d143934-075f-43da-be88-0c3ba4c29b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the average 'time(min)' for each meal class\n",
    "avg_time_by_class = recipes.groupby('meal_class')['time(min)'].mean().reset_index()\n",
    "\n",
    "# Step 2: Standardize 'time(min)' using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "recipes['time(min)_scaled'] = scaler.fit_transform(recipes[['time(min)']])\n",
    "\n",
    "# Create a DataFrame to store cosine similarity values\n",
    "cosine_similarity_df = pd.DataFrame()\n",
    "\n",
    "# Calculate cosine similarity for each recipe with respect to its meal class average\n",
    "cosine_similarity_values = []\n",
    "\n",
    "for index, row in recipes.iterrows():\n",
    "    meal_class = row['meal_class']\n",
    "    avg_time = avg_time_by_class[avg_time_by_class['meal_class'] == meal_class]['time(min)'].values[0]\n",
    "\n",
    "    # Calculate cosine similarity for the recipe\n",
    "    cosine_sim = cosine_similarity(\n",
    "        [[row['time(min)_scaled']]],\n",
    "        [[avg_time]])  # Use the standardized average time\n",
    "\n",
    "    cosine_similarity_values.append(cosine_sim[0][0])\n",
    "\n",
    "# Add cosine similarity values to the DataFrame\n",
    "cosine_similarity_df['cosine_sim'] = cosine_similarity_values\n",
    "\n",
    "# Add cosine similarity values to the main 'recipes' DataFrame\n",
    "recipes['time_cosine'] = cosine_similarity_df['cosine_sim']\n",
    "\n",
    "recipes.drop(columns=['time(min)_scaled'], inplace=True)\n",
    "\n",
    "# Save the scalers using pickle\n",
    "with open('../scalers/scalers_time.pkl', 'wb') as scaler_file:\n",
    "    pickle.dump(scaler, scaler_file)\n",
    "\n",
    "recipes.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a04f20-ce83-47ff-9b99-9a8be2633e4d",
   "metadata": {},
   "source": [
    "## Scaling and Finding Cosine Similarity in Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ac0efe-b746-4689-841e-72e7bb10a35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping\n",
    "cost_mapping = {'Económico': 1, 'Médio': 2, 'Dispendioso': 3}\n",
    "\n",
    "# Use the map function to create the 'cost_mapped' column\n",
    "recipes['cost_mapped'] = recipes['cost'].map(cost_mapping)\n",
    "recipes['cost_mapped'] = recipes['cost_mapped'].astype(int)\n",
    "\n",
    "# Step 1: Calculate the average 'cost_mapped' for each meal class\n",
    "avg_cost_by_class = recipes.groupby('meal_class')['cost_mapped'].mean().reset_index()\n",
    "\n",
    "# Step 2: Calculate cosine similarity\n",
    "# Standardize 'cost_mapped' using StandardScaler to avoid issues with different scales\n",
    "scaler = StandardScaler()\n",
    "recipes['cost_scaled'] = scaler.fit_transform(recipes[['cost_mapped']])\n",
    "\n",
    "# Calculate cosine similarity for each recipe within its meal class\n",
    "def cosine_similarity_to_avg(row):\n",
    "    meal_class = row['meal_class']\n",
    "    avg_cost = avg_cost_by_class[avg_cost_by_class['meal_class'] == meal_class]['cost_mapped'].values[0]\n",
    "    cosine_sim = cosine_similarity([[row['cost_scaled']]], [[avg_cost]])[0][0]\n",
    "    return cosine_sim\n",
    "\n",
    "recipes['cost_cosine'] = recipes.apply(cosine_similarity_to_avg, axis=1)\n",
    "\n",
    "# Drop the columns that were used for calculations but are no longer needed\n",
    "columns_to_drop = ['cost_mapped', 'cost_scaled']\n",
    "recipes.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Save the scalers using pickle\n",
    "with open('../scalers/scalers_cost.pkl', 'wb') as scaler_file:\n",
    "    pickle.dump(scaler, scaler_file)\n",
    "\n",
    "recipes.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d39f811-1ad7-4904-bdc3-6e3808c6304b",
   "metadata": {},
   "source": [
    "## Scaling and Finding Cosine Similarity in Servings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e8cf79-0474-4964-9245-c39219729749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the average 'servings' for each meal class\n",
    "avg_servings_by_class = recipes.groupby('meal_class')['servings'].mean().reset_index()\n",
    "\n",
    "# Step 2: Calculate cosine similarity for 'servings' within each meal class\n",
    "def cosine_similarity_to_avg_servings(row):\n",
    "    meal_class = row['meal_class']\n",
    "    avg_servings = avg_servings_by_class[avg_servings_by_class['meal_class'] == meal_class]['servings'].values[0]\n",
    "    cosine_sim = cosine_similarity([[row['servings']]], [[avg_servings]])[0][0]\n",
    "    return cosine_sim\n",
    "\n",
    "# Standardize 'servings' using StandardScaler to avoid issues with different scales\n",
    "scaler = StandardScaler()\n",
    "recipes['servings_scaled'] = scaler.fit_transform(recipes[['servings']])\n",
    "\n",
    "# Calculate cosine similarity for each recipe within its meal class and create a new column\n",
    "recipes['servings_cosine'] = recipes.apply(cosine_similarity_to_avg_servings, axis=1)\n",
    "\n",
    "# Drop the 'servings_scaled' column as it's no longer needed\n",
    "recipes.drop(columns=['servings_scaled'], inplace=True)\n",
    "\n",
    "# Save the scaler using pickle\n",
    "with open('../scalers/scalers_servings.pkl', 'wb') as scaler_file:\n",
    "    pickle.dump(scaler, scaler_file)\n",
    "\n",
    "recipes.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0412af-3f8f-4314-9e45-d7bf884e6199",
   "metadata": {},
   "source": [
    "## Scaling and Finding Cosine Similarity in Rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08bb161-b4a5-406d-a64c-a870ec64f7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Calculate the average 'rating' for each meal class\n",
    "avg_rating_by_class = recipes.groupby('meal_class')['rating'].mean().reset_index()\n",
    "\n",
    "# Step 2: Calculate cosine similarity for 'rating' within each meal class\n",
    "def cosine_similarity_to_avg_rating(row):\n",
    "    meal_class = row['meal_class']\n",
    "    avg_rating = avg_rating_by_class[avg_rating_by_class['meal_class'] == meal_class]['rating'].values[0]\n",
    "    cosine_sim = cosine_similarity([[row['rating']]], [[avg_rating]])[0][0]\n",
    "    return cosine_sim\n",
    "\n",
    "# Standardize 'rating' using StandardScaler to avoid issues with different scales\n",
    "scaler = StandardScaler()\n",
    "recipes['rating_scaled'] = scaler.fit_transform(recipes[['rating']])\n",
    "\n",
    "# Calculate cosine similarity for each recipe within its meal class and create a new column\n",
    "recipes['rating_cosine'] = recipes.apply(cosine_similarity_to_avg_rating, axis=1)\n",
    "\n",
    "# Drop the 'rating_scaled' column as it's no longer needed\n",
    "recipes.drop(columns=['rating_scaled'], inplace=True)\n",
    "\n",
    "# Save the scaler using pickle\n",
    "with open('../scalers/scalers_rating.pkl', 'wb') as scaler_file:\n",
    "    pickle.dump(scaler, scaler_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48474312-9a5a-43ce-9534-050fc3b41a6f",
   "metadata": {},
   "source": [
    "## Scaling and Finding Cosine Similarity in Difficulty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28882528-abce-4a1f-b466-2e22a5f297c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping\n",
    "difficulty_mapping = {'Fácil': 1, 'Médio': 2, 'Difícil': 3}\n",
    "\n",
    "# Use the map function to create the 'difficulty_mapped' column\n",
    "recipes['difficulty_mapped'] = recipes['difficulty'].map(difficulty_mapping)\n",
    "\n",
    "# Step 1: Calculate the average 'cost_mapped' for each meal class\n",
    "avg_difficulty_by_class = recipes.groupby('meal_class')['difficulty_mapped'].mean().reset_index()\n",
    "\n",
    "# Step 2: Calculate cosine similarity\n",
    "# Standardize 'cost_mapped' using StandardScaler to avoid issues with different scales\n",
    "scaler = StandardScaler()\n",
    "recipes['difficulty_scaled'] = scaler.fit_transform(recipes[['difficulty_mapped']])\n",
    "\n",
    "# Calculate cosine similarity for each recipe within its meal class\n",
    "def cosine_similarity_to_avg(row):\n",
    "    meal_class = row['meal_class']\n",
    "    avg_difficulty = avg_difficulty_by_class[avg_difficulty_by_class['meal_class'] == meal_class]['difficulty_mapped'].values[0]\n",
    "    cosine_sim = cosine_similarity([[row['difficulty_scaled']]], [[avg_difficulty]])[0][0]\n",
    "    return cosine_sim\n",
    "\n",
    "recipes['difficulty_cosine'] = recipes.apply(cosine_similarity_to_avg, axis=1)\n",
    "\n",
    "# Drop the columns that were used for calculations but are no longer needed\n",
    "columns_to_drop = ['difficulty_mapped', 'difficulty_scaled']\n",
    "recipes.drop(columns=columns_to_drop, inplace=True)\n",
    "\n",
    "# Save the scalers using pickle\n",
    "with open('../scalers/scalers_difficulty.pkl', 'wb') as scaler_file:\n",
    "    pickle.dump(scaler, scaler_file)\n",
    "\n",
    "recipes.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b3e01c-26f8-48ba-885b-7a4c37b97a11",
   "metadata": {},
   "source": [
    "# Creating a Column of the Avg of Each Row's Cosine Similarity against the Avg of its Meal CLass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40386d7-de3d-4e3b-9ea4-a883b0e7d5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455322b2-4d1c-4764-8392-a71b709d6d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes['avg_cosine'] = recipes[['difficulty_cosine', 'rating_cosine', 'servings_cosine', 'difficulty_cosine', 'cost_cosine', 'time_cosine', 'preparations_cosine', 'ingredients_cosine']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79a612a-9bea-43a2-9ccf-8907d7d4f58b",
   "metadata": {},
   "source": [
    "# Saving File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdff8083-83cb-4f30-8c5c-8b534f69d684",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes.to_csv('../data/clean/recipes.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea24d345-b37f-48e6-80b1-4e60b779c3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a0c31c-e055-42e8-8e4b-f2b5c02249e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7d4069-f217-451c-8924-d516541fe13f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finalproj",
   "language": "python",
   "name": "finalproj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
