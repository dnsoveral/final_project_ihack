{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18d345a7-ea8f-443c-91b8-ac795e6f4c24",
   "metadata": {
    "tags": []
   },
   "source": [
    "![logo_ironhack_blue 7](https://user-images.githubusercontent.com/23629340/40541063-a07a0a8a-601a-11e8-91b5-2f13e4e6b441.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c4516b-5631-4f41-82ab-b8b7b7b085cf",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df8cf23-9f5d-4099-b095-35e491d8b055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85886e6b-20b7-462d-889e-d970f913a108",
   "metadata": {},
   "source": [
    "# Calling CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94de8c5-be6b-464e-b0b8-525133abd8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_links = pd.read_csv('../data/raw/recipe_links.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1695c5ba-25f5-476f-a094-99e6ea7ff579",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_links"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b4be4f-c5ae-4fc5-ba77-7fa1322ac352",
   "metadata": {},
   "source": [
    "# Full Scrapping for Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4107aa7-3edc-4b3e-8054-9fb9bd684cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_recipe_features(df, total_expected_recipes):\n",
    "    \n",
    "    \"\"\"\n",
    "    Scrapes recipe features from URLs in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): DataFrame containing a 'link' column with recipe URLs.\n",
    "        total_expected_recipes (int): Total number of products to download.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the original data and the scraped recipe features.\n",
    "    \"\"\"\n",
    "    \n",
    "    start_time = time.time()\n",
    "    loops_counter = 0\n",
    "    loops_before_sleep = 100\n",
    "    scraped_data = []\n",
    "    search_url = 0\n",
    "    redirect_count = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        try:\n",
    "            \n",
    "            url = row['link']\n",
    "        \n",
    "            title = df.at[index, 'title']\n",
    "        \n",
    "            search_url += 1\n",
    "        \n",
    "            \n",
    "            response = requests.get(url)\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            rating = soup.find('span', class_=\"wprm-recipe-rating-average\")\n",
    "            rating = rating.text if rating else ''\n",
    "            rating_final = f'{rating} out of 5'\n",
    "\n",
    "            difficulty = soup.find('span', class_='wprm-recipe-de_dificuldade wprm-block-text-normal')\n",
    "            difficulty = difficulty.text if difficulty else ''\n",
    "\n",
    "            cost = soup.find('span', class_='wprm-recipe-_da_refeio wprm-block-text-normal')\n",
    "            cost = cost.text if cost else ''\n",
    "            \n",
    "            img_element = soup.select_one('.td-post-featured-image img.entry-thumb')\n",
    "            if img_element:\n",
    "                img_url = img_element['src']\n",
    "        \n",
    "            hours_element_prep = soup.find(\"span\", class_=\"wprm-recipe-details wprm-recipe-details-hours wprm-recipe-prep_time wprm-recipe-prep_time-hours\")\n",
    "            minutes_element_prep = soup.find(\"span\", class_=\"wprm-recipe-details wprm-recipe-details-minutes wprm-recipe-prep_time wprm-recipe-prep_time-minutes\")\n",
    "            hours_prep = hours_element_prep.text if hours_element_prep else \"0\"\n",
    "            minutes_prep = minutes_element_prep.text if minutes_element_prep else \"0\"\n",
    "\n",
    "            prep_time = f\"{hours_prep}h {minutes_prep}min\"\n",
    "\n",
    "            hours_element_total = soup.find('span', class_='wprm-recipe-details wprm-recipe-details-hours wprm-recipe-total_time wprm-recipe-total_time-hours')\n",
    "            minutes_element_total = soup.find(\"span\", class_=\"wprm-recipe-details wprm-recipe-details-minutes wprm-recipe-total_time wprm-recipe-total_time-minutes\")\n",
    "            hours_total = hours_element_total.text if hours_element_total else \"0\"\n",
    "            minutes_total = minutes_element_total.text if minutes_element_total else \"0\"\n",
    "\n",
    "            total_time = f\"{hours_total}h {minutes_total}min\"\n",
    "\n",
    "            meal_class = soup.find(\"span\", class_=\"wprm-recipe-course wprm-block-text-normal\")\n",
    "            meal_class = meal_class.get_text(strip = True) if meal_class else ''\n",
    "\n",
    "            servings = soup.find('span', class_='wprm-recipe-servings-adjustable-tooltip')\n",
    "            servings = servings.get_text(strip = True) if servings else \"0\"\n",
    "        \n",
    "            preparation = [step.text for step in soup.find_all(\"div\", class_=\"wprm-recipe-instruction-text\")]\n",
    "\n",
    "            ingredient_data = []\n",
    "\n",
    "            ingredients = soup.find_all('li', class_='wprm-recipe-ingredient')\n",
    "            \n",
    "            for ing in ingredients:\n",
    "                amount_element = ing.find(\"span\", class_=\"wprm-recipe-ingredient-amount\")\n",
    "                unit_element = ing.find(\"span\", class_=\"wprm-recipe-ingredient-unit\")\n",
    "                name_element = ing.find(\"a\")\n",
    "                notes_faded_element = ing.find(\"span\", class_=\"wprm-recipe-ingredient-notes wprm-recipe-ingredient-notes-faded\")\n",
    "\n",
    "                amount = amount_element.text if amount_element else '0'\n",
    "    \n",
    "                # Check if notes faded are present and swap name and unit if they exist\n",
    "                if notes_faded_element:\n",
    "                    unit = name_element.text if name_element else '-'\n",
    "                    name = notes_faded_element.text\n",
    "                else:\n",
    "                    unit = unit_element.text if unit_element else '-'\n",
    "                    name = name_element.text if name_element else '-'\n",
    "\n",
    "                ingredient_data.append({\n",
    "                    'Amount': amount,\n",
    "                    'Unit': unit,\n",
    "                    'Name': name,\n",
    "                    })\n",
    "\n",
    "                  # Create a DataFrame for the ingredient data\n",
    "                ingredient_df = pd.DataFrame(ingredient_data)\n",
    "\n",
    "                scraped_data.append({\n",
    "                'Meal Class': meal_class,\n",
    "                'Difficulty': difficulty,\n",
    "                'Cost': cost,\n",
    "                'Rating': rating_final,\n",
    "                'Title': title,\n",
    "                'Prep Time': prep_time, \n",
    "                'Total Time': total_time,\n",
    "                'Servings': servings,\n",
    "                'Ingredient_Amount': amount,\n",
    "                'Ingredient_Unit': unit,\n",
    "                'Ingredient_Name': name,\n",
    "                'Preparations': preparation,\n",
    "                'Recipe Link': url,  \n",
    "                'Image URL': img_url})\n",
    "                \n",
    "            # Calculate and display progress\n",
    "        \n",
    "            percentage_downloaded = search_url / total_expected_recipes * 100\n",
    "            current_time = time.time() - start_time\n",
    "            print(f'Time: {int(current_time // 60):02d}:{int(current_time % 60):02d}.', end='\\r')\n",
    "            print()\n",
    "            print(f'Scraping URL {url}.', end='\\r')\n",
    "            print()\n",
    "            print(f'Recipes Downloaded: {search_url}/{total_expected_recipes} ({percentage_downloaded:.2f}%).', end='\\r')\n",
    "            print()\n",
    "            \n",
    "            # Check if it's time for a sleep\n",
    "            if loops_counter < loops_before_sleep:\n",
    "                loops_counter += 1\n",
    "            else:\n",
    "                sleep_time = 45\n",
    "                random_sleep_timer = random.randint(int(sleep_time * 0.5), int(sleep_time * 1.5))\n",
    "                print(f'Sleeping for {random_sleep_timer} s...', end='\\r')\n",
    "                time.sleep(random_sleep_timer)\n",
    "                loops_counter = 0  # Reset the counter after sleeping\n",
    "            clear_output(wait=True)     \n",
    "                \n",
    "        except requests.exceptions.TooManyRedirects:\n",
    "            print(f\"Too many redirects for URL: {url}. Skipping...\")\n",
    "            continue  # Skip this URL and continue with the next iteration\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error accessing URL: {url}. Error: {e}\")\n",
    "            continue  # Skip this URL and continue with the next iteration\n",
    "    \n",
    "    combined_df = pd.DataFrame(scraped_data)   \n",
    "    \n",
    "    # Printing Download Summary\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    downloaded_percentage = (search_url) / (total_expected_recipes) * 100\n",
    "    print(f'Recipe download complete. Total: ({search_url}/{total_expected_recipes} {downloaded_percentage:.2f}%)')\n",
    "    print(f'Total Running Time: {int(total_time // 60):02d}:{int(total_time % 60):02d}')\n",
    "\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff4160f-814c-4e8c-9124-3cdebf213a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_expected_recipes = len(recipe_links)\n",
    "recipes_clean = scrape_recipe_features(recipe_links, total_expected_recipes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d28b53-cbd6-4656-8d10-857fcaa57985",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedeef13-bfc9-4a1f-b718-0c5fb4166926",
   "metadata": {},
   "source": [
    "# Saving CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145d3984-176b-4488-a613-e0a5ec0f8b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes_clean.to_csv('../data/raw/recipes_clean.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finalproj",
   "language": "python",
   "name": "finalproj"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
